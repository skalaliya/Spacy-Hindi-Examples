{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization with SpaCy\n",
    "Tokenization is the process of splitting text into individual words, punctuation marks, or other meaningful elements called tokens. SpaCy provides an easy-to-use interface for tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blank Language Model: spacy.blank(\"en\") creates a blank language object for English. \n",
    "\n",
    "This includes a tokenizer but no other pipeline components (like named entity recognition or part-of-speech tagging).\n",
    "\n",
    "The loop prints each token from the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "sentence\n",
      "is\n",
      "typically\n",
      "associated\n",
      "with\n",
      "a\n",
      "clause\n",
      ".\n",
      "A\n",
      "clause\n",
      "can\n",
      "either\n",
      "be\n",
      "a\n",
      "clause\n",
      "simplex\n",
      "or\n",
      "a\n",
      "clause\n",
      "complex\n",
      ".\n",
      "A\n",
      "clause\n",
      "simplex\n",
      "represents\n",
      "a\n",
      "single\n",
      "process\n",
      "going\n",
      "on\n",
      "through\n",
      "time\n",
      ".\n",
      "A\n",
      "clause\n",
      "complex\n",
      "represents\n",
      "a\n",
      "logical\n",
      "relation\n",
      "between\n",
      "two\n",
      "or\n",
      "more\n",
      "processes\n",
      "and\n",
      "is\n",
      "thus\n",
      "composed\n",
      "of\n",
      "two\n",
      "or\n",
      "more\n",
      "clause\n",
      "simplexes\n",
      ".\n",
      "There\n",
      "may\n",
      "be\n",
      "the\n",
      "sentences\n",
      "which\n",
      "talks\n",
      "about\n",
      "currentcy\n",
      "such\n",
      "as\n",
      "$\n",
      "Pound\n",
      "and\n",
      "Euro\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank English language model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Tokenize a sentence\n",
    "doc = nlp(\"A sentence is typically associated with a clause.A clause can either be a clause simplex or a clause complex. A clause simplex represents a single process going on through time. A clause complex represents a logical relation between two or more processes and is thus composed of two or more clause simplexes. There may be the sentences which talks about currentcy such as $ Pound and Euro\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Accessing Tokens by Index\n",
    "You can access individual tokens by their index in the Doc object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Tokens and Their Attributes\n",
    "You can access tokens by index, and each token has several attributes like is_alpha, is_currency, like_num, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence\n",
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "token = doc[1]  # Access the second token\n",
    "print(token.text)  # Output: Strange\n",
    "\n",
    "# List of all token attributes\n",
    "print(dir(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Token Attributes\n",
    "SpaCy tokens have various attributes like is_alpha, like_num, and is_currency to check if a token is a word, a number, or a currency symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method-wrapper '__init__' of spacy.tokens.token.Token object at 0x16c191530>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0.__init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore various attributes of tokens to understand their properties.\n",
    "Example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tony gave two $ to Peter.\")\n",
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extracting Information from Text\n",
    "You can extract specific types of tokens (e.g., email addresses) by iterating over the tokens and checking their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Span Objects\n",
    "A Span is a slice of the Doc object and can be created by slicing.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony gave two $ to\n"
     ]
    }
   ],
   "source": [
    "span = doc[0:5]\n",
    "print(span)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Email IDs\n",
    "Using like_email attribute, you can extract email addresses from text.\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virat@kohli.com', 'maria@sharapova.com', 'serena@williams.com', 'joe@root.com']\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Dayton high school, 8th grade students information\n",
    "Name  birth day   email\n",
    "Virat   5 June, 1882    virat@kohli.com\n",
    "Maria  12 April, 2001  maria@sharapova.com\n",
    "Serena  24 June, 1998   serena@williams.com \n",
    "Joe      1 May, 1997    joe@root.com\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "emails = [token.text for token in doc if token.like_email]\n",
    "print(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Support for Multiple Languages\n",
    "SpaCy supports many languages. You can tokenize text in other languages just by changing the language code (e.g., hi for Hindi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "राजेंद्र False\n",
      "प्रसाद False\n",
      ", False\n",
      "भारत False\n",
      "के False\n",
      "पहले False\n",
      "राष्ट्रपति False\n",
      ", False\n",
      "दो False\n",
      "कार्यकाल False\n",
      "के False\n",
      "लिए False\n",
      "कार्यालय False\n",
      "रखने False\n",
      "वाले False\n",
      "एकमात्र False\n",
      "व्यक्ति False\n",
      "हैं False\n",
      "। False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"hi\")\n",
    "doc = nlp(\"राजेंद्र प्रसाद, भारत के पहले राष्ट्रपति, दो कार्यकाल के लिए कार्यालय रखने वाले एकमात्र व्यक्ति हैं।\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Customizing Tokenizer\n",
    "You can customize the tokenizer by adding special cases. For example, splitting \"gimme\" into \"gim\" and \"me\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [{ORTH: \"gim\"}, {ORTH: \"me\"}])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Sentence Segmentation\n",
    "Sentence Tokenization involves splitting the text into sentences. The default blank model does not include sentence boundary detection. You need to add a Sentencizer component to enable this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenization (Segmentation)\n",
    "To split text into sentences, you need to add a component like sentencizer to the pipeline.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of Mumbai.\n",
      "Hulk loves chat of Delhi\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of Mumbai. Hulk loves chat of Delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Exercise Solutions\n",
    "(1) Extracting URLs from Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.data.gov/', 'http://www.science', 'http://data.gov.uk/.', 'http://www3.norc.org/gss+website/', 'http://www.europeansocialsurvey.org/.']\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "urls = [token.text for token in doc if token.like_url]\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Extracting Money Transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two $', '500 €']\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc = nlp(transactions)\n",
    "\n",
    "money_transactions = []\n",
    "for token in doc:\n",
    "    if token.is_currency:\n",
    "        money_transactions.append(f\"{doc[token.i - 1].text} {token.text}\")\n",
    "\n",
    "print(money_transactions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Tokenization: ['Dr.', 'Strange', 'loves', 'pav', 'bhaji', 'of', 'mumbai', 'as', 'it', 'costs', 'only', '2', '$', 'per', 'plate', '.']\n",
      "Access Token by Index: Strange\n",
      "Token Attributes: [('Dr.', False, False, False), ('Strange', True, False, False), ('loves', True, False, False), ('pav', True, False, False), ('bhaji', True, False, False), ('of', True, False, False), ('mumbai', True, False, False), ('as', True, False, False), ('it', True, False, False), ('costs', True, False, False), ('only', True, False, False), ('2', False, True, False), ('$', False, False, True), ('per', True, False, False), ('plate', True, False, False), ('.', False, False, False)]\n",
      "Extract Emails: ['virat@kohli.com', 'maria@sharapova.com']\n",
      "Custom Tokenizer: ['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']\n",
      "Sentence Segmentation: ['Dr. Strange loves pav bhaji of mumbai.', 'Hulk loves chat of delhi']\n",
      "Extract URLs: ['http://www.data.gov/', 'http://data.gov.uk/.']\n",
      "Extract Money Transactions: ['two $', '500 €']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "# Initialize blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Example 1: Basic Tokenization\n",
    "def basic_tokenization(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Example 2: Accessing Tokens by Index\n",
    "def access_token_by_index(sentence, index):\n",
    "    doc = nlp(sentence)\n",
    "    return doc[index].text\n",
    "\n",
    "# Example 3: Token Attributes\n",
    "def token_attributes(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    attributes = [(token.text, token.is_alpha, token.like_num, token.is_currency) for token in doc]\n",
    "    return attributes\n",
    "\n",
    "# Example 4: Extracting Emails from Text\n",
    "def extract_emails(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if token.like_email]\n",
    "\n",
    "# Example 5: Customizing Tokenizer\n",
    "def customize_tokenizer(sentence):\n",
    "    nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "        {ORTH: \"gim\"},\n",
    "        {ORTH: \"me\"},\n",
    "    ])\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Example 6: Sentence Segmentation\n",
    "def sentence_segmentation(text):\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# Exercise 1: Extracting URLs\n",
    "def extract_urls(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if token.like_url]\n",
    "\n",
    "# Exercise 2: Extracting Money Transactions\n",
    "def extract_money_transactions(transactions):\n",
    "    doc = nlp(transactions)\n",
    "    money_transactions = []\n",
    "    for token in doc:\n",
    "        if token.is_currency:\n",
    "            money_transactions.append(f\"{doc[token.i - 1].text} {token.text}\")\n",
    "    return money_transactions\n",
    "\n",
    "# Usage examples\n",
    "if __name__ == \"__main__\":\n",
    "    sentence = \"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\"\n",
    "    print(\"Basic Tokenization:\", basic_tokenization(sentence))\n",
    "    \n",
    "    print(\"Access Token by Index:\", access_token_by_index(sentence, 1))\n",
    "    \n",
    "    print(\"Token Attributes:\", token_attributes(sentence))\n",
    "    \n",
    "    email_text = '''\n",
    "    Virat   5 June, 1882    virat@kohli.com\n",
    "    Maria   12 April, 2001  maria@sharapova.com\n",
    "    '''\n",
    "    print(\"Extract Emails:\", extract_emails(email_text))\n",
    "    \n",
    "    custom_sentence = \"gimme double cheese extra large healthy pizza\"\n",
    "    print(\"Custom Tokenizer:\", customize_tokenizer(custom_sentence))\n",
    "    \n",
    "    segmentation_text = \"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\"\n",
    "    print(\"Sentence Segmentation:\", sentence_segmentation(segmentation_text))\n",
    "    \n",
    "    url_text = '''\n",
    "    Look for data at http://www.data.gov/ and http://data.gov.uk/.\n",
    "    '''\n",
    "    print(\"Extract URLs:\", extract_urls(url_text))\n",
    "    \n",
    "    transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "    print(\"Extract Money Transactions:\", extract_money_transactions(transactions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
