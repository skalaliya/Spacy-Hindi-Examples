{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some more examples of how you can use spaCy with Hindi language text. \n",
    "\n",
    "These examples cover a variety of NLP tasks like tokenization, entity recognition (if supported), custom tokenization, and handling different attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation and Integration\n",
    "\n",
    "Basic Tokenization shows how spaCy tokenizes Hindi text into individual words and punctuation marks.\n",
    "\n",
    "Token Attributes demonstrate accessing different properties of tokens, like is_alpha (if the token is alphabetic), like_num (if the token resembles a number), and is_currency (if the token is a currency symbol).\n",
    "\n",
    "Custom Tokenization is particularly useful for handling idiomatic expressions or compound words in Hindi.\n",
    "\n",
    "Currency Detection examples illustrate how to extract and work with financial data in Hindi text.\n",
    "\n",
    "Sentence Segmentation splits Hindi text into sentences, which can be important for further processing like summarization or dialogue systems.\n",
    "\n",
    "Stop Words example shows how to remove common words that might not be useful for text analysis.\n",
    "\n",
    "POS Tagging and Entity Recognition are powerful features for understanding the grammatical structure and identifying key entities in the text, though they may require trained models.\n",
    "\n",
    "Pattern Matching is useful for extracting specific phrases or patterns from Hindi text.\n",
    "\n",
    "These examples provide a comprehensive overview of spaCy's capabilities with the Hindi language, showing how you can perform various NLP tasks such as tokenization, custom token handling, and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Tokenization in Hindi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in the sentence:\n",
      "यहां\n",
      "गर्मी\n",
      "बहुत\n",
      "हो\n",
      "रही\n",
      "है\n",
      ",\n",
      "लेकिन\n",
      "बारिश\n",
      "की\n",
      "कोई\n",
      "उम्मीद\n",
      "नहीं\n",
      "है\n",
      "।\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank Hindi language object\n",
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "# Tokenize a Hindi sentence\n",
    "doc = nlp(\"यहां गर्मी बहुत हो रही है, लेकिन बारिश की कोई उम्मीद नहीं है।\")\n",
    "\n",
    "# Print tokens\n",
    "print(\"Tokens in the sentence:\")\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Token Attributes in Hindi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token attributes:\n",
      "Token: राम, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: ने, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: 5000, is_alpha: False, like_num: True, is_currency: False\n",
      "Token: ₹, is_alpha: False, like_num: False, is_currency: True\n",
      "Token: दिए, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: ,, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: जबकि, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: श्याम, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: ने, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: 3000, is_alpha: False, like_num: True, is_currency: False\n",
      "Token: ₹, is_alpha: False, like_num: False, is_currency: True\n",
      "Token: उधार, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: लिए, is_alpha: False, like_num: False, is_currency: False\n",
      "Token: ।, is_alpha: False, like_num: False, is_currency: False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"राम ने 5000 ₹ दिए, जबकि श्याम ने 3000 ₹ उधार लिए।\")\n",
    "\n",
    "print(\"\\nToken attributes:\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, is_alpha: {token.is_alpha}, like_num: {token.like_num}, is_currency: {token.is_currency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Tokenization in Hindi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pre-Processing the Text\n",
    "\n",
    "Before passing the text to the tokenizer, you can manually replace \"भैया जी\" with \"भैया जी\" where each word is separate. \n",
    "\n",
    "This way, the tokenizer will naturally split them without needing a special case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after pre-processing: ['भैया', 'जी', 'नमस्ते', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank Hindi language object\n",
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "# Pre-process the text to separate \"भैया जी\"\n",
    "text = \"भैया जी नमस्ते!\"\n",
    "text = text.replace(\"भैया जी\", \"भैया जी\")\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens after pre-processing:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Custom Tokenizer with Infixes\n",
    "\n",
    "Modify the tokenizer's infix rules to handle such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after custom tokenizer: ['भैया', 'जी', 'नमस्ते!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# Create a blank Hindi language object\n",
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "# Modify the tokenizer to handle specific infixes\n",
    "infixes = list(nlp.Defaults.infixes) + [r\"(?<=भैया)(?=जी)\"]\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "\n",
    "doc = nlp(\"भैया जी नमस्ते!\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens after custom tokenizer:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Manual Tokenization for Specific Cases\n",
    "\n",
    "Manually split tokens for specific phrases after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after manual processing: ['भैया', 'जी', 'नमस्ते', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank Hindi language object\n",
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "doc = nlp(\"भैया जी नमस्ते!\")\n",
    "tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text == \"भैया जी\":\n",
    "        tokens.extend([\"भैया\", \"जी\"])\n",
    "    else:\n",
    "        tokens.append(token.text)\n",
    "\n",
    "print(\"Tokens after manual processing:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code will not work because the error I'm encountering, \n",
    "\n",
    "[E997] Tokenizer special cases are not allowed to modify the text, occurs \n",
    "\n",
    "Because the special case I'm trying to add would result in a modification of the original text, which is not allowed by spaCy's tokenizer.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "In the special case you're trying to add:\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"भैया जी\", [{ORTH: \"भैया\"}, {ORTH: \"जी\"}])\n",
    "\n",
    "spaCy expects that adding this special case should not change the actual text that is tokenized. \n",
    "\n",
    "However, the way this special case is defined, it would modify the text, which results in the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.symbols import ORTH\n",
    "\n",
    "# Customizing the tokenizer to split \"भैया जी\" into \"भैया\" and \"जी\"\n",
    "#nlp.tokenizer.add_special_case(\"भैया जी\", [{ORTH: \"भैया\"}, {ORTH: \"जी\"}])\n",
    "\n",
    "#oc = nlp(\"भैया जी नमस्ते!\")\n",
    "#tokens = [token.text for token in doc]\n",
    "#print(\"\\nCustom tokenizer tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Detecting Currency in Hindi Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currency detection in the sentence:\n",
      "Currency: ₹, Previous Token: 5000\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"मोहन ने 5000 ₹ दिए, जबकि सोहन ने 100 डॉलर दिए।\")\n",
    "\n",
    "print(\"\\nCurrency detection in the sentence:\")\n",
    "for token in doc:\n",
    "    if token.is_currency:\n",
    "        print(f\"Currency: {token.text}, Previous Token: {doc[token.i - 1].text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extracting Numeric Values and Associated Currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting numeric values and associated currency:\n",
      "Amount: 2000 ₹\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"राजू ने 2000 ₹ उधार लिए, जबकि सुमन ने 50 डॉलर का भुगतान किया।\")\n",
    "\n",
    "print(\"\\nExtracting numeric values and associated currency:\")\n",
    "for i, token in enumerate(doc):\n",
    "    if token.like_num:\n",
    "        # Check if the next token is currency\n",
    "        if doc[i + 1].is_currency:\n",
    "            print(f\"Amount: {token.text} {doc[i + 1].text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sentence Segmentation (Sentence Tokenization) in Hindi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences in the text:\n",
      "मोहन मुझसे मिलने आए थे।\n",
      "उन्होंने कहा कि वह कल वापस जाएंगे।\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "doc = nlp(\"मोहन मुझसे मिलने आए थे। उन्होंने कहा कि वह कल वापस जाएंगे।\")\n",
    "print(\"\\nSentences in the text:\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Using Stop Words in Hindi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop words in Hindi (Custom List):\n",
      "{'इसका', 'उसि', 'मैं', 'वहिं', 'उंहें', 'कोई', 'कि', 'साबुत', 'इन्हीं', 'उन', 'उसके', 'भीतर', 'अगर', 'जेसा', 'वगेरह', 'साभ', 'के', 'वहां', 'करें', 'अदि', 'वाले', 'तिंहें', 'अंदर', 'कहते', 'या', 'वहीं', 'जा', 'ना', 'जिस', 'करते', 'ऐसे', 'करता', 'जैसा', 'थी', 'जितना', 'रखें', 'जहाँ', 'वुह', 'संग', 'वग़ैरह', 'भी', 'एसे', 'किया', 'में', 'इसकि', 'जिन्हें', 'इंहिं', 'इनका', 'व', 'निहायत', 'मेरा', 'जेसे', 'कोनसा', 'इन्हें', 'मगर', 'यहाँ', 'उसे', 'तिन्हों', 'होने', 'अभि', 'होते', 'उन्हीं', 'यह', 'सारा', 'यिह', 'रवासा', 'कर', 'उस', 'बनी', 'तिंहों', 'थि', 'अपनी', 'हुई', 'बनि', 'किन्हों', 'किसी', 'घर', 'दूसरे', 'द्वारा', 'नहिं', 'वरग', 'सकते', 'होती', 'ऱ्वासा', 'वर्ग', 'हुआ', 'का', 'तिन्हें', 'तिन', 'इंहें', 'आदि', 'दुसरा', 'काफ़ी', 'नहीं', 'हुए', 'लिये', 'निचे', 'कितना', 'कई', 'दिया', 'किंहें', 'सभी', 'एवं', 'तिसे', 'जब', 'अप', 'हो', 'किसि', 'वे', 'जैसे', 'नीचे', 'जिधर', 'उसी', 'इतयादि', 'अत', 'पे', 'बहुत', 'लेकिन', 'सकता', 'उन्हें', 'जिसे', 'हे', 'कुल', 'मुझको', 'सो', 'इसि', 'जिन', 'साथ', 'इसमें', 'करना', 'यहि', 'उनकि', 'तक', 'काफि', 'पर', 'होना', 'दवारा', 'जो', 'मानो', 'था', 'उन्हों', 'बाला', 'अभी', 'कइ', 'हि', 'सभि', 'इन्हों', 'किस', 'इसी', 'ओर', 'उनका', 'पहले', 'किर', 'मे', 'लिए', 'कोइ', 'इंहों', 'और', 'यदि', 'वहाँ', 'अपनि', 'जीधर', 'कौनसा', 'तिस', 'इत्यादि', 'पूरा', 'यहां', 'हें', 'से', 'जिन्हों', 'होति', 'इस', 'जहां', 'अपने', 'इसकी', 'थे', 'ही', 'कहा', 'कौन', 'होता', 'फिर', 'बाद', 'दुसरे', 'हुइ', 'ये', 'उंहिं', 'उनको', 'को', 'इसे', 'तरह', 'न', 'एक', 'किसे', 'तब', 'तो', 'सबसे', 'हूँ', 'हैं', 'गया', 'आप', 'बही', 'अपना', 'बिलकुल', 'भि', 'जिंहें', 'भितर', 'उनकी', 'उंहों', 'कोन', 'है', 'ने', 'दो', 'कुछ', 'जिंहों', 'किन्हें', 'किंहों', 'बहि', 'दबारा', 'पुरा', 'वह', 'उनके', 'हुअ', 'इसके', 'रहा', 'की', 'एस', 'इन', 'करने', 'रहे', 'यही'}\n",
      "\n",
      "Filtering out stop words from the text:\n",
      "['राम', 'श्याम', 'दोनों', 'अच्छे', 'दोस्त', '।']\n"
     ]
    }
   ],
   "source": [
    "# Manually adding stop words (spaCy may not have a pre-defined stop word list for Hindi)\n",
    "from spacy.lang.hi.stop_words import STOP_WORDS\n",
    "\n",
    "print(\"\\nStop words in Hindi (Custom List):\")\n",
    "print(STOP_WORDS)\n",
    "\n",
    "doc = nlp(\"राम और श्याम दोनों अच्छे दोस्त हैं।\")\n",
    "\n",
    "print(\"\\nFiltering out stop words from the text:\")\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Part-of-Speech (POS) Tagging in Hindi (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part-of-Speech tags:\n",
      "Token: मोहन, POS: \n",
      "Token: ने, POS: \n",
      "Token: पुस्तक, POS: \n",
      "Token: पढ़ी, POS: \n",
      "Token: ।, POS: \n"
     ]
    }
   ],
   "source": [
    "# Note: POS tagging may not be supported in the default blank Hindi model, but if supported:\n",
    "\n",
    "# Assuming the POS tagger is available (might need a trained model)\n",
    "doc = nlp(\"मोहन ने पुस्तक पढ़ी।\")\n",
    "print(\"\\nPart-of-Speech tags:\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, POS: {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Entity Recognition in Hindi (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities in the text:\n"
     ]
    }
   ],
   "source": [
    "# Note: Entity recognition may not be supported in the default blank Hindi model, but if supported:\n",
    "\n",
    "doc = nlp(\"मुंबई भारत की आर्थिक राजधानी है।\")\n",
    "\n",
    "print(\"\\nNamed Entities in the text:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Matching Patterns in Hindi Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matcher in spaCy relies on part-of-speech (POS) tags to match patterns based on POS attributes. \n",
    "\n",
    "The blank Hindi language model (nlp = spacy.blank(\"hi\")) doesn't include a POS tagger, which is why you may see error.\n",
    "\n",
    "## Solutions\n",
    "\n",
    "1. Add a Tagger to the Pipeline:\n",
    "\n",
    "You need to add a morphologizer or a tagger along with an attribute_ruler to the pipeline. \n",
    "However, keep in mind that spaCy’s default blank model for Hindi might not have pre-trained weights available for these components.\n",
    "\n",
    "2. Use Patterns Without POS Tags:\n",
    "\n",
    "If you do not require POS tags, you can modify the pattern to use simpler matching criteria, such as text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Without POS Tags\n",
    "\n",
    "Here’s how you can modify the example to avoid using POS tags:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Pattern: This pattern searches for \"राम ने\" followed by any alphabetic token (IS_ALPHA: True). This avoids the need for POS tagging.\n",
    "\n",
    "Matcher: The matcher object is used to find the pattern in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pattern matching results:\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize a blank Hindi language model\n",
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "# Initialize matcher with the Hindi vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match \"राम ने\" followed by any word\n",
    "pattern = [{\"TEXT\": \"राम\"}, {\"TEXT\": \"ने\"}, {\"IS_ALPHA\": True}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"RAM_ACTION\", [pattern])\n",
    "\n",
    "doc = nlp(\"राम ने खाना खाया और फिर सो गया।\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"\\nPattern matching results:\")\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(f\"Matched Span: {span.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Tagger\n",
    "\n",
    "If you do want to use POS tags, you would need to train or load a Hindi model that includes a POS tagger. \n",
    "\n",
    "However, this requires access to a pre-trained Hindi model that supports POS tagging, which spaCy might not provide out of the box for Hindi.\n",
    "\n",
    "Here's how you would do it if you had such a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import spacy\\nfrom spacy.pipeline import Tagger, AttributeRuler\\nfrom spacy.matcher import Matcher\\n\\n# Load a model with a tagger or create a new one and add components\\nnlp = spacy.blank(\"hi\")\\ntagger = Tagger(nlp.vocab)\\nruler = AttributeRuler(nlp)\\nnlp.add_pipe(tagger)\\nnlp.add_pipe(ruler)\\n\\n# Initialize matcher with the Hindi vocabulary\\nmatcher = Matcher(nlp.vocab)\\n\\n# Define a pattern to match \"राम ने\" followed by a verb\\npattern = [{\"TEXT\": \"राम\"}, {\"TEXT\": \"ने\"}, {\"POS\": \"VERB\"}]\\n\\n# Add the pattern to the matcher\\nmatcher.add(\"RAM_ACTION\", [pattern])\\n\\ndoc = nlp(\"राम ने खाना खाया और फिर सो गया।\")\\n\\nmatches = matcher(doc)\\n\\nprint(\"\\nPattern matching results:\")\\nfor match_id, start, end in matches:\\n    span = doc[start:end]\\n    print(f\"Matched Span: {span.text}\")\\n '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import spacy\n",
    "from spacy.pipeline import Tagger, AttributeRuler\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model with a tagger or create a new one and add components\n",
    "nlp = spacy.blank(\"hi\")\n",
    "tagger = Tagger(nlp.vocab)\n",
    "ruler = AttributeRuler(nlp)\n",
    "nlp.add_pipe(tagger)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "# Initialize matcher with the Hindi vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match \"राम ने\" followed by a verb\n",
    "pattern = [{\"TEXT\": \"राम\"}, {\"TEXT\": \"ने\"}, {\"POS\": \"VERB\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"RAM_ACTION\", [pattern])\n",
    "\n",
    "doc = nlp(\"राम ने खाना खाया और फिर सो गया।\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"\\nPattern matching results:\")\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(f\"Matched Span: {span.text}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
